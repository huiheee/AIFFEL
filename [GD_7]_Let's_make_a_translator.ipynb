{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 문자 단위의 번역기"
      ],
      "metadata": {
        "id": "Z1suW7lXN-h3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.  데이터 전처리\n",
        "\n",
        "1. 필요한 라이브러리 불러오기\n",
        "2. 데이터 판다스로 불러오기\n",
        "\n",
        "3. 불필요한 열 삭제하고, 훈련데이터 5만개로 줄이기\n",
        "\n",
        "4. 디코더에 시작 토큰 \\t (<sos>), 종료 토큰 \\n (<eos>) 추가하기\n",
        "5. Tokenizer(단어 사전, 단어장) 생성하기\n",
        "6. 단어장의 크기를 변수에 담아주기(0번 토큰도 있으므로 +1하기)\n",
        "7. 영어 데이터, 프랑스어 데이터들의 최대 길이 및 기본 통계 정보 구하기(패딩하기 위해)\n",
        "8. 프랑스어 데이터, 즉 프랑스어 시퀀스를 2가지 버전(입력 데이터, 출력 데이터)으로 준비\n",
        "\n",
        "**왜 프랑스어 시퀀스는 2가지 버전으로 나누어 준비?**\n",
        "\n",
        "- 디코더의 출력과 비교할 정답 데이터로 사용 → <SOS> 토큰 필요 없음\n",
        "- 교사 강요를 위해 디코더의 입력으로 사용 → <EOS> 토큰 필요 없음\n",
        "\n",
        "1. 영어 데이터, 프랑스어 입력 데이터, 프랑스어 출력 데이터 각각에 패딩\n",
        "2. 원핫 인코딩 진행\n",
        "3. 훈련용 데이터, 검증용 데이터 나누어주기"
      ],
      "metadata": {
        "id": "2qRotU5WoYGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요 라이브러리 불러오기\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "gh_plPZIn_h_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 판다스로 불러오기\n",
        "import os\n",
        "file_path = os.getenv('HOME')+'/aiffel/translator_seq2seq/data/fra.txt'\n",
        "lines = pd.read_csv(file_path, names=['eng', 'fra', 'cc'], sep='\\t')\n",
        "print('전체 샘플의 수 :',len(lines))\n",
        "lines.sample(5) #샘플 5개 출력"
      ],
      "metadata": {
        "id": "jNFpGpL6n_cN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 불필요한 열 삭제하고 훈련데이터 5만개로 줄이기\n",
        "lines = lines[['eng', 'fra']][:50000] # 5만개 샘플 사용\n",
        "lines.sample(5)"
      ],
      "metadata": {
        "id": "6ThwmAeNn_Wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 디코더에 시작 토큰, 종료 토큰 추가하기\n",
        "sos_token = '\\t'\n",
        "eos_token = '\\n'\n",
        "lines.fra = lines.fra.apply(lambda x : '\\t '+ x + ' \\n')\n",
        "print('전체 샘플의 수 :',len(lines))\n",
        "lines.sample(5)"
      ],
      "metadata": {
        "id": "QsbNY-tDn_P2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_tokenizer = Tokenizer(char_level=True)   # 문자 단위로 Tokenizer를 생성합니다.\n",
        "eng_tokenizer.fit_on_texts(lines.eng)               # 50000개의 행을 가진 eng의 각 행에 토큰화를 수행\n",
        "input_text = eng_tokenizer.texts_to_sequences(lines.eng)    # 단어를 숫자값 인덱스로 변환하여 저장\n",
        "input_text[:3]\n",
        "print(input_text)"
      ],
      "metadata": {
        "id": "bphSTWEHn_Kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fra_tokenizer = Tokenizer(char_level=True)   # 문자 단위로 Tokenizer를 생성합니다.\n",
        "fra_tokenizer.fit_on_texts(lines.fra)                 # 50000개의 행을 가진 fra의 각 행에 토큰화를 수행\n",
        "target_text = fra_tokenizer.texts_to_sequences(lines.fra)     # 단어를 숫자값 인덱스로 변환하여 저장\n",
        "target_text[:3]"
      ],
      "metadata": {
        "id": "7i1J-YR2n_Fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
        "print('영어 단어장의 크기 :', eng_vocab_size)\n",
        "print('프랑스어 단어장의 크기 :', fra_vocab_size)"
      ],
      "metadata": {
        "id": "Yu3Uoa0Gn_CT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_eng_seq_len = max([len(line) for line in input_text])\n",
        "max_fra_seq_len = max([len(line) for line in target_text])\n",
        "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
        "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
      ],
      "metadata": {
        "id": "EdroWVu_n-_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('전체 샘플의 수 :',len(lines))\n",
        "print('영어 단어장의 크기 :', eng_vocab_size)\n",
        "print('프랑스어 단어장의 크기 :', fra_vocab_size)\n",
        "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
        "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
      ],
      "metadata": {
        "id": "Evgj7KWLn-8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input = input_text\n",
        "# 종료 토큰 제거\n",
        "decoder_input = [[ char for char in line if char != fra_tokenizer.word_index[eos_token] ] for line in target_text]\n",
        "# 시작 토큰 제거\n",
        "decoder_target = [[ char for char in line if char != fra_tokenizer.word_index[sos_token] ] for line in target_text]"
      ],
      "metadata": {
        "id": "EkFLmgG0n-5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(decoder_input[:3])\n",
        "print(decoder_target[:3])"
      ],
      "metadata": {
        "id": "fgMnglvQn-z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input = pad_sequences(encoder_input, maxlen = max_eng_seq_len, padding='post')\n",
        "decoder_input = pad_sequences(decoder_input, maxlen = max_fra_seq_len, padding='post')\n",
        "decoder_target = pad_sequences(decoder_target, maxlen = max_fra_seq_len, padding='post')\n",
        "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
        "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
        "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
      ],
      "metadata": {
        "id": "7prSDXken-xQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoder_input[0])"
      ],
      "metadata": {
        "id": "g_UIaeYGw3PE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input = to_categorical(encoder_input)\n",
        "decoder_input = to_categorical(decoder_input)\n",
        "decoder_target = to_categorical(decoder_target)\n",
        "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
        "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
        "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
      ],
      "metadata": {
        "id": "ezmp-OtTw3MA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_of_val = 3000\n",
        "\n",
        "encoder_input_train = encoder_input[:-n_of_val]\n",
        "decoder_input_train = decoder_input[:-n_of_val]\n",
        "decoder_target_train = decoder_target[:-n_of_val]\n",
        "\n",
        "encoder_input_test = encoder_input[-n_of_val:]\n",
        "decoder_input_test = decoder_input[-n_of_val:]\n",
        "decoder_target_test = decoder_target[-n_of_val:]\n",
        "\n",
        "print('영어 학습데이터의 크기(shape) :',np.shape(encoder_input))\n",
        "print('프랑스어 학습 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
        "print('프랑스어 학습 출력데이터의 크기(shape) :',np.shape(decoder_target))"
      ],
      "metadata": {
        "id": "5wuy-ko5w3JW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 모델 훈련"
      ],
      "metadata": {
        "id": "LXdC71mV5TvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "print('⏳')"
      ],
      "metadata": {
        "id": "8SFO86Qi17pF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력 텐서 생성.\n",
        "encoder_inputs = Input(shape=(None, eng_vocab_size))\n",
        "# hidden size가 256인 인코더의 LSTM 셀 생성\n",
        "encoder_lstm = LSTM(units = 256, return_state = True)\n",
        "# 디코더로 전달할 hidden state, cell state를 리턴. encoder_outputs은 여기서는 불필요.\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
        "# hidden state와 cell state를 다음 time step으로 전달하기 위해서 별도 저장.\n",
        "encoder_states = [state_h, state_c]"
      ],
      "metadata": {
        "id": "pCRYTXAww3GS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력 텐서 생성.\n",
        "decoder_inputs = Input(shape=(None, fra_vocab_size))\n",
        "# hidden size가 256인 인코더의 LSTM 셀 생성\n",
        "decoder_lstm = LSTM(units = 256, return_sequences = True, return_state=True)\n",
        "# decoder_outputs는 모든 time step의 hidden state\n",
        "decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state = encoder_states)"
      ],
      "metadata": {
        "id": "w7-kr1h9w3DF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_softmax_layer = Dense(fra_vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_softmax_layer(decoder_outputs)"
      ],
      "metadata": {
        "id": "1jYdXufMw2-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import utils\n",
        "\n",
        "utils.plot_model(model)"
      ],
      "metadata": {
        "id": "5uAmz88k8HOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "cM4OH8Er5Wox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
        "          validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
        "          batch_size=128, epochs=50)"
      ],
      "metadata": {
        "id": "WyWntvbk5WlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 모델 테스트"
      ],
      "metadata": {
        "id": "CIZZ2jOAn9H9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gi34OOxc5Whf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j3xLFsmL5Wdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pzFx-ML65Wai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 단어 Level로 번역기 업그레이드하기\n",
        "Rubric\n",
        "\n",
        "|평가문항|상세기준|\n",
        "|---|---|\n",
        "1. 번역기 모델 학습에 필요한 텍스트 데이터 전처리가 잘 이루어졌다.|구두점, 대소문자, 띄어쓰기 등 번역기 모델에 요구되는 전처리가 정상적으로 진행되었다.|\n",
        "|2. seq2seq 기반의 번역기 모델이 정상적으로 구동된다.|seq2seq 모델 훈련결과를 그래프로 출력해보고, validation loss그래프가 우하향하는 경향성을 보이며 학습이 진행됨이 확인되었다.|\n",
        "|3. 테스트 결과 의미가 통하는 수준의 번역문이 생성되었다.|테스트용 디코더 모델이 정상적으로 만들어졌으며, input(영어)와 output(프랑스어) 모두 한글로 번역해서 결과를 출력해보았고, 둘의 내용이 유사함을 확인하였다.|\n"
      ],
      "metadata": {
        "id": "k8gQQqolNKK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 위 실습에서 구현한 번역기는 글자 단위(chracter-level)에서 구현된 번역기였으나, 실제로는 단어 단위(Word-level)에서 구현되는 것이 더 보편적이다.\n",
        "- 진행될 프로젝트에서는 동일한 데이터셋을 사용하면서 글자 단위와는 다른 전처리와 to_categorical()함수가 아닌 임베팅 층(Embedding layer)를 추가하여 단어 단위의 번역기를 완성시키기\n",
        "- 단어 단위로 할 경우, 단어의 개수가 글자 단위로 했을 경우와 비교하여 단어장의 크기도 커지고 학습 속도도 더 느려지게 된다.  따라서 학습과 테스트 시의 원활한 진행을 위해 데이터에서 상위 33,000개의 샘플만 사용하기("
      ],
      "metadata": {
        "id": "2PYhx-8eODcj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6U5VgzCM9Wh"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "\n",
        "print(tensorflow.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step1.정제, 정규화, 전처리(영어, 프랑스어)"
      ],
      "metadata": {
        "id": "a-Si5C9MNwen"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VeuEZzWcNg8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H5vNd-K_Ng5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M9Q_k9jvNg3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z55NCANaNg0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6I3O0lWkNgyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8ySiq70sNgvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6xhJT5XqNgtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LBBZX4QHNgrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dXUHnouWNgph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zaF4I9y1NgnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zAeokWIUNgjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LGdTDgB1NggH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yxPHErH2Ngdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dS-3t7agNgbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3-Dl_emqNgY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GtF9rOeANgV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fotwIsJNNgRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0bBMExCQNgOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gy4_DYneNgKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jL0xfQhoNgIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 회고"
      ],
      "metadata": {
        "id": "Wjm2JCJ8SwOZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reference\n",
        "\n",
        "- LSTM (https://simpling.tistory.com/19), (https://wikidocs.net/106473)\n",
        "-"
      ],
      "metadata": {
        "id": "MDbNF-mFSxRG"
      }
    }
  ]
}